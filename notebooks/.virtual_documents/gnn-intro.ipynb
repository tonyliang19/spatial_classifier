from IPython.lib.display import YouTubeVideo

import numpy as np

import torch
import torch.nn as nn
import torch.optim as optim

from torch_geometric.data import Data, GraphSAINTRandomWalkSampler
from torch_geometric.datasets import Planetoid, Entities
from torch_geometric.nn import GCNConv, RGCNConv, GATConv, SAGEConv, JumpingKnowledge, GINConv, DeepGraphInfomax

torch.manual_seed(200620)
np.random.seed(200620)


YouTubeVideo('cWIeTMklzNg', height=500, width=890, start=70, end=460)


print('For this tutorial, we will use a standard citation dataset that is commonly used to benchmark GNN performance.')
print('The Cora dataset is a homogeneous, undirected graph where nodes are publications linked by citations.')
dataset = Planetoid(root='/tmp/Cora', name='Cora', split='full')
print('It contains:')
graph = dataset[0]
print('\t- {:d} labels'.format(dataset.num_classes))
print('\t- {:d} nodes'.format(graph.num_nodes))
print('\t- {:d} training'.format(graph.train_mask.sum().item()))
print('\t- {:d} validation'.format(graph.val_mask.sum().item()))
print('\t- {:d} testing'.format(graph.test_mask.sum().item()))


class Hyperparameters():
    def __init__(self):
        self.num_node_features = None
        self.num_classes = None
        self.lr = 0.005
        self.w_decay = 5e-4   
        self.dropout = 0.3
        self.epochs = 200                
        self.cuda = True                
        self.device  =  None    

args = Hyperparameters()
args.num_node_features = graph.num_node_features
args.num_classes = dataset.num_classes
args.cuda = args.cuda and torch.cuda.is_available() 
if args.cuda:
    args.device = torch.device('cuda:0') 
else:
    args.device = torch.device('cpu')


args.device


class LearnGraph(): 
    
    def __init__(self, graph, model, args, criterion=None):
        self.args = args
        self.graph = graph.to(self.args.device)
        self.model = model.to(self.args.device)
        
        if not criterion: 
            criterion = nn.CrossEntropyLoss()
        self.criterion = criterion
        
        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.args.lr, weight_decay=self.args.w_decay)
        
        self.train_loss = []
        self.val_loss = []
        self.train_complete = False 
        
    def learn(self) -> None:
        # tracks training and validation loss over epochs
        # can add early stopping mechanism by comparing losses
        for epoch in range(self.args.epochs): 
            if self.train_complete: return
            
            tl = self.train_epoch()
            self.train_loss.append(tl)
            
            vl = self.val()
            self.val_loss.append(vl)
                
        self.train_complete = True
        
    def train_epoch(self) -> float:
        # trains a single epoch (ie. one pass over the full graph) and updates the models parameters
        # returns the loss
        self.model.train()
        labels = self.graph.y[self.graph.train_mask]
        self.optim.zero_grad()
        output = self.model.forward(self.graph) 
        loss = self.criterion(output[self.graph.train_mask], labels)
        loss.backward()
        self.optim.step()
        return loss.data.item()
    
    def val(self) -> float:
        # returns the validation loss 
        self.model.eval()
        labels = self.graph.y[self.graph.val_mask]
        output = self.model.forward(self.graph) 
        loss = self.criterion(output[self.graph.val_mask], labels)
        return loss.data.item()
    
    def test(self) -> float: 
        # returns the test accuracy 
        if not self.train_complete: 
            self.learn()
        self.model.eval()
        labels = self.graph.y[self.graph.test_mask]    
        _, pred = self.model.forward(self.graph).max(dim=1)
        correct = float ( pred[self.graph.test_mask].eq(labels).sum().item() )
        acc = correct / self.graph.test_mask.sum().item()
        return acc
        



class GNN(torch.nn.Module):
    def __init__(self): 
        super(GNN, self).__init__()
        self.conv1 = GCNConv(args.num_node_features, 64)
        self.conv2 = GCNConv(64, args.num_classes)
        self.transition = nn.Sequential(
            nn.ReLU(),
            nn.Dropout(p=args.dropout)
        )

    def forward(self, graph): 
        x, edge_index = graph.x, graph.edge_index
        x = self.conv1(x, edge_index)
        x = self.transition(x)
        x = self.conv2(x, edge_index)
        return x

learner = LearnGraph(model=GNN(), graph=graph, args=args)
acc = learner.test()
print('Accuracy: {:.1%}'.format(acc))


mutag_dataset = Entities(root='/tmp/MUTAG', name='MUTAG')
mutag_graph = mutag_dataset[0]

class LearnMUTAG(): 
    
    def __init__(self, graph, model, args, criterion=None):
        self.args = args
        self.graph = graph.to(self.args.device)
        self.model = model.to(self.args.device)
        
        if not criterion: 
            criterion = nn.CrossEntropyLoss()
        self.criterion = criterion
        
        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.args.lr, weight_decay=self.args.w_decay)
        
        self.train_loss = []
        self.train_complete = False 
        
    def learn(self) -> None:
        for epoch in range(self.args.epochs): 
            if self.train_complete: return
            tl = self.train_epoch()
            self.train_loss.append(tl)
        self.train_complete = True
        
    def train_epoch(self) -> float:
        self.model.train()
        labels = self.graph.train_y
        self.optim.zero_grad()
        output = self.model.forward(self.graph) 
        loss = self.criterion(output[self.graph.train_idx], labels)
        loss.backward()
        self.optim.step()
        return loss.data.item()
    
    def test(self) -> float: 
        # returns the test accuracy 
        if not self.train_complete: 
            self.learn()
        self.model.eval()
        labels = self.graph.test_y
        _, pred = self.model.forward(self.graph).max(dim=1)
        correct = float ( pred[self.graph.test_idx].eq(labels).sum().item() )
        acc = correct / len(self.graph.test_idx)
        return acc
    
class GNN(torch.nn.Module):
    def __init__(self): 
        super(GNN, self).__init__()
        self.conv1 = RGCNConv(mutag_graph.num_nodes, 16, mutag_dataset.num_relations, num_bases=30)
        self.conv2 = RGCNConv(16, mutag_dataset.num_classes, mutag_dataset.num_relations, num_bases=30)
        self.transition = nn.Sequential(
            nn.ReLU(),
            nn.Dropout(p=args.dropout)
        )

    def forward(self, graph): 
        x, edge_index, edge_type = graph.x, graph.edge_index, graph.edge_type
        x = self.conv1(x, edge_index, edge_type)
        x = self.transition(x)
        x = self.conv2(x, edge_index, edge_type)
        return x

learner = LearnMUTAG(model=GNN(), graph=mutag_graph, args=args)
acc = learner.test()
print('Accuracy: {:.1%}'.format(acc))


class GNN(torch.nn.Module):
    def __init__(self): 
        super(GNN, self).__init__()
        self.conv1 = GATConv(args.num_node_features, 8, heads=8)
        self.conv2 = GATConv(64, args.num_classes, heads=1)
        self.transition = nn.Sequential(
            nn.ReLU(),
            nn.Dropout(p=args.dropout)
        )

    def forward(self, graph): 
        x, edge_index = graph.x, graph.edge_index
        x = self.conv1(x, edge_index)
        x = self.transition(x)
        x = self.conv2(x, edge_index)
        return x

learner = LearnGraph(model=GNN(), graph=graph, args=args)
acc = learner.test()
print('Accuracy: {:.1%}'.format(acc))


class GNN(torch.nn.Module):
    def __init__(self): 
        super(GNN, self).__init__()
        self.conv1 = SAGEConv(args.num_node_features, 64, normalize=True)
        self.conv1.aggr = 'max'
        self.conv2 = SAGEConv(64, args.num_classes,  normalize=True)
        self.conv2.aggr = 'max'
        self.transition = nn.Sequential(
            nn.ReLU(),
            nn.Dropout(p=args.dropout)
        )
        print()

    def forward(self, graph): 
        x, edge_index = graph.x, graph.edge_index
        x = self.conv1(x, edge_index)
        x = self.transition(x)
        x = self.conv2(x, edge_index)
        return x

learner = LearnGraph(model=GNN(), graph=graph, args=args)
acc = learner.test()
print('Accuracy: {:.1%}'.format(acc))


class GNN(torch.nn.Module):
    def __init__(self): 
        super(GNN, self).__init__()
        self.conv1 = GCNConv(args.num_node_features, 64)
        self.convx= GCNConv(64, 64)
        self.jk = JumpingKnowledge(mode='max')
        self.final = nn.Linear(64, args.num_classes)
        self.transition = nn.Sequential(
            nn.ReLU(),
            nn.Dropout(p=args.dropout)
        )

    def forward(self, graph): 
        x, edge_index = graph.x, graph.edge_index
        xs = []
        x = self.conv1(x, edge_index)
        x = self.transition(x)
        xs.append(x)
        for _ in range(5): 
            x = self.convx(x, edge_index)
            x = self.transition(x)
            xs.append(x)
        x = self.jk(xs)
        x = self.final(x)
        return x

learner = LearnGraph(model=GNN(), graph=graph, args=args)
acc = learner.test()
print('Accuracy: {:.1%}'.format(acc))


class GNN(torch.nn.Module):
    def __init__(self): 
        super(GNN, self).__init__()
        self.transition = nn.Sequential(
            nn.ReLU(),
            nn.Dropout(p=args.dropout)
        )
        self.mlp1 = nn.Sequential(
            nn.Linear(args.num_node_features, 256), 
            nn.ReLU(), 
            nn.Linear(256, 64), 
        )
        self.conv1 = GINConv(self.mlp1)
        self.mlp2 = nn.Sequential(
            nn.Linear(64, 16), 
            nn.ReLU(),
            nn.Linear(16, args.num_classes), 
        )
        self.conv2= GINConv(self.mlp2)
        
        
    def forward(self, graph): 
        x, edge_index = graph.x, graph.edge_index
        x = self.conv1(x, edge_index)
        x = self.transition(x)
        x = self.conv2(x, edge_index)
        return x
        
learner = LearnGraph(model=GNN(), graph=graph, args=args)
acc = learner.test()
print('Accuracy: {:.1%}'.format(acc))       


class LearnDeepGraphInfomax(): 
    
    def __init__(self, graph, enc_dgi, enc_cls, args, criterion=None): 
        self.args = args
        self.graph = graph.to(self.args.device)
        
        self.dgi_model = DeepGraphInfomax(enc_dgi.hidden_ch, enc_dgi, enc_dgi.summary, enc_dgi.corruption)
        self.dgi_model = self.dgi_model.to(self.args.device)
        
        self.cls_model = enc_cls.to(self.args.device)
        
        if not criterion: 
            criterion = nn.CrossEntropyLoss()
        self.criterion = criterion
        
        parameters = [*self.dgi_model.parameters()] + [*self.cls_model.parameters()]
        self.optim = torch.optim.Adam(parameters, lr=self.args.lr, weight_decay=self.args.w_decay)
        
        self.train_loss = []
        self.val_loss = []
        self.train_complete = False
        
    def learn(self) -> None: 
        for epoch in range(self.args.epochs): 
            if self.train_complete: return
            
            tl = self.train_epoch()
            self.train_loss.append(tl)
            
            vl = self.val()
            self.val_loss.append(vl)
                
        self.train_complete = True   
        
    def train_epoch(self) -> float:
        self.dgi_model.train()
        self.cls_model.train()
        labels = self.graph.y[self.graph.train_mask]
        self.optim.zero_grad()
        pos_z, neg_z, summary = self.dgi_model.forward(x=self.graph.x, edge_index=self.graph.edge_index)
        output = self.cls_model.forward(pos_z, self.graph.edge_index)
        loss = self.dgi_model.loss(pos_z, neg_z, summary) + self.criterion(output[self.graph.train_mask], labels)
        loss.backward()
        self.optim.step()
        return loss.data.item()
    
    def val(self) -> float: 
        self.dgi_model.eval()
        self.cls_model.eval()
        labels = self.graph.y[self.graph.val_mask]
        pos_z, neg_z, summary = self.dgi_model.forward(self.graph.x, self.graph.edge_index)
        output = self.cls_model.forward(pos_z, self.graph.edge_index)
        loss = self.dgi_model.loss(pos_z, neg_z, summary) + self.criterion(output[self.graph.val_mask], labels)
        return loss.data.item()
    
    def test(self) -> float: 
        if not self.train_complete: 
            self.learn()
        self.dgi_model.eval()
        self.cls_model.eval()
        labels = self.graph.y[self.graph.test_mask]   
        pos_z, neg_z, summary = self.dgi_model.forward(self.graph.x, self.graph.edge_index)
        _, pred = self.cls_model.forward(pos_z, self.graph.edge_index).max(dim=1)
        correct = float ( pred[self.graph.test_mask].eq(labels).sum().item() )
        acc = correct / self.graph.test_mask.sum().item()
        return acc


class Encoder_DGI(torch.nn.Module):
    def __init__(self, hidden_ch=64): 
        super(Encoder_DGI, self).__init__()
        self.hidden_ch = hidden_ch
        self.conv = GCNConv(args.num_node_features, hidden_ch)
        self.activation = nn.PReLU()
        
    def corruption(self, x, edge_index): 
        # corrupted features are obtained by row-wise shuffling of the original features 
        # corrupted graph consists of the same nodes but located in different places 
        return x[torch.randperm(x.size(0))], edge_index
        
    def summary(self, z, *args, **kwargs): 
        return torch.sigmoid(z.mean(dim=0))

    def forward(self, x, edge_index): 
        x = self.conv(x, edge_index)
        x = self.activation(x)
        return x 


class Encoder_CLS(torch.nn.Module):
    def __init__(self, hidden_ch=64): 
        super(Encoder_CLS, self).__init__()
        self.conv = GCNConv(hidden_ch, args.num_classes)

    def forward(self, x, edge_index): 
        return self.conv(x, edge_index)


learner = LearnDeepGraphInfomax(enc_dgi=Encoder_DGI(), enc_cls=Encoder_CLS(), graph=graph, args=args)
acc = learner.test()
print('Accuracy: {:.1%}'.format(acc))


class LearnGraphSAINT(): 
    
    def __init__(self, graph, model, args, criterion=None):
        self.args = args
        self.graph = graph.to(self.args.device)
        self.model = model.to(self.args.device)
        
        self.loader =  GraphSAINTRandomWalkSampler(self.graph, batch_size=100, walk_length=2, num_steps=self.args.epochs)
        
        if not criterion: 
            criterion = nn.CrossEntropyLoss()
        self.criterion = criterion
        
        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.args.lr, weight_decay=self.args.w_decay)
        
        self.train_loss = []
        self.val_loss = []
        self.train_complete = False 
        
    def learn(self) -> None: 
        
        for epoch, batch in enumerate(self.loader): 
            if self.train_complete: return
        
            tl = self.train_batch(batch)
            self.train_loss.append(tl)
            
            vl = self.val()
            self.val_loss.append(vl)
                
        self.train_complete = True
        
    def train_batch(self, batch) -> float:
        self.model.train()
        labels = batch.y[batch.train_mask]
        self.optim.zero_grad()
        output = self.model.forward(batch) 
        loss = self.criterion(output[batch.train_mask], labels)
        loss.backward()
        self.optim.step()
        return loss.data.item()
    
    def val(self) -> float: 
        self.model.eval()
        labels = self.graph.y[self.graph.val_mask]
        output = self.model.forward(self.graph) 
        loss = self.criterion(output[self.graph.val_mask], labels)
        return loss.data.item()
    
    def test(self) -> float: 
        if not self.train_complete: 
            self.learn()
        self.model.eval()
        labels = self.graph.y[self.graph.test_mask]    
        _, pred = self.model.forward(self.graph).max(dim=1)
        correct = float ( pred[self.graph.test_mask].eq(labels).sum().item() )
        acc = correct / self.graph.test_mask.sum().item()
        return acc


class GNN(torch.nn.Module):
    def __init__(self): 
        super(GNN, self).__init__()
        self.conv1 = GCNConv(args.num_node_features, 64)
        self.conv2 = GCNConv(64, args.num_classes)
        self.transition = nn.Sequential(
            nn.ReLU(),
            nn.Dropout(p=args.dropout)
        )

    def forward(self, graph): 
        x, edge_index = graph.x, graph.edge_index
        x = self.conv1(x, edge_index)
        x = self.transition(x)
        x = self.conv2(x, edge_index)
        return x

learner = LearnGraphSAINT(model=GNN(), graph=graph, args=args)
acc = learner.test()
print('Accuracy: {:.1%}'.format(acc))
